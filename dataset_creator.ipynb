{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/srmt99/stock-market/blob/master/dataset_creator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import glob\n",
    "import os\n",
    "import copy\n",
    "import random as rnd\n",
    "import datetime\n",
    "mpl.rcParams['figure.figsize'] = (6, 4)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### hyper_Parameters ######\n",
    "data_set_num = 9\n",
    "start_from_market = 100\n",
    "plt.figure(figsize=(10,6))\n",
    "test_split = 0.2\n",
    "future_prediction = 1\n",
    "corr_w = 40\n",
    "w = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the data from github into colab's working space\n",
    "!wget https://github.com/srmt99/stock-market/blob/master/data/markets.npy?raw=true\n",
    "!wget https://github.com/srmt99/stock-market/blob/master/data/stocks_1.npy?raw=true\n",
    "!wget https://github.com/srmt99/stock-market/blob/master/data/stocks_2.npy?raw=true\n",
    "\n",
    "stocks = []\n",
    "for i in np.load(\"stocks_1.npy?raw=true\",alow_pickle=True):\n",
    "    stocks.append(i)\n",
    "for i in np.load(\"stocks_2.npy?raw=true\",allow_pickle=True):\n",
    "    stocks.append(i)\n",
    "stocks = np.array(stocks)\n",
    "markets = np.load(\"markets.npy?raw=true\",allow_pickle=True)\n",
    "                  \n",
    "# extracting minimum market length\n",
    "min_market_len = len(markets[0])\n",
    "for i in markets[1:]:\n",
    "  if len(i)<min_market_len:\n",
    "    min_market_len = len(i)\n",
    "\n",
    "# making all markets of the same length\n",
    "for i in range(len(markets)):\n",
    "  while len(markets[i])>min_market_len:\n",
    "    markets[i] = np.delete(markets[i],0,0)\n",
    "\n",
    "for i in range(len(stocks)):\n",
    "  while len(stocks[i])>min_market_len:\n",
    "    stocks[i] = np.delete(stocks[i],0,0)\n",
    "\n",
    "markets = np.stack(markets,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploting some random stock prices and markets\n",
    "r1 = np.random.randint(len(stocks))\n",
    "r2 = np.random.randint(len(markets))\n",
    "print(r1,r2)\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(stocks[r1][:,1],label=\"stock prices\")\n",
    "plt.plot(markets[r2][:,1],label=\"market\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_to_windows(input,kernel,future):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in range(kernel,len(input)-future):\n",
    "        data.append(input[i-kernel:i,:])\n",
    "        labels.append(input[i:i+future,1])\n",
    "    return np.array(data),np.array(labels).reshape(len(labels),future)\n",
    "\n",
    "def turn_to_windows_multi(input,kernel):\n",
    "  data = []\n",
    "  for i in range(kernel,input.shape[1]-1):\n",
    "        data.append(input[:,i-kernel:i])\n",
    "  return np.array(data)\n",
    "\n",
    "def smooth(input):\n",
    "    output = []\n",
    "    output.append(input[0])\n",
    "    output.append(np.mean([input[0],input[1]]))\n",
    "    for i in range(2,len(input)-2):\n",
    "        mean = np.mean([input[i-2],input[i-1],input[i],input[i+1],input[i+2]])\n",
    "        output.append(mean)\n",
    "    output.append(np.mean([input[len(input)-2],input[len(input)-1]]))\n",
    "    output.append(input[len(input)-1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count,stock in enumerate(stocks):\n",
    "  records = []\n",
    "  labels = []\n",
    "  correlations = []\n",
    "  x,y = turn_to_windows(stock,corr_w,1)\n",
    "  for wc,window in enumerate(x[:,:,1]):\n",
    "    corr = np.corrcoef(window,markets[: , min_market_len - len(x) - corr_w - 1 + wc : min_market_len - len(x) + wc -1 , 1])[1:,0]\n",
    "    correlations.append( np.nan_to_num(corr) )\n",
    "  correlations = np.array(correlations)\n",
    "  x,y = turn_to_windows(stock,w,1)\n",
    "  x2 = turn_to_windows_multi(markets[:,:,1],w)\n",
    "  for wc in range(len(correlations),w,-1):\n",
    "      record = np.zeros( (2*len(markets)+5,w) )\n",
    "      record[:5,:] = np.transpose(x[wc + (len(x)-len(correlations)-1) ][:,1:]) # part 1\n",
    "      record[5:5+len(markets)] = np.transpose(correlations[wc-w:wc]) # part 2\n",
    "      record[5+len(markets):5+2*len(markets)] = x2[wc + (len(x2)-len(correlations)-1)] # part 3\n",
    "      records.append(record)\n",
    "      labels.append(y[wc + (len(x)-len(correlations)-1) ])\n",
    "  np.save(f\"records_{count}\",np.array([records,labels]))\n",
    "  print(f\"{count}/{len(stocks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "labels = []\n",
    "for count,filename in enumerate(glob.glob(\"records_*.npy\")):\n",
    "  x,y = np.load(filename,allow_pickle=True)\n",
    "  for i in x:\n",
    "    train.append(i)\n",
    "  for i in y:\n",
    "    labels.append(i[0])\n",
    "  if count == 30:\n",
    "    break\n",
    "\n",
    "train = np.array(train)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "We split data into 3 parts: train, validation, test\n",
    "We control the number of records in each set using\n",
    "test_split and val_split\n",
    "\"\"\"\n",
    "val_split, test_split = 0.1, 0.2\n",
    "data = train\n",
    "num_val, num_test = int(val_split * data.shape[0]), int(test_split * data.shape[0])\n",
    "\n",
    "train_x = data[:-(num_val + num_test)]\n",
    "train_y = labels[:-(num_val + num_test)]\n",
    "val_x = data[-(num_val + num_test):-num_test]\n",
    "val_y = labels[-(num_val + num_test):-num_test]\n",
    "test_x = data[-num_test:]\n",
    "test_y = labels[-num_test:]\n",
    "\n",
    "print('train_x:', train_x.shape)\n",
    "print('train_y:', train_y.shape)\n",
    "print('val_x:', val_x.shape)\n",
    "print('val_y:', val_y.shape)\n",
    "print('test_x:', test_x.shape)\n",
    "print('test_y:', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell we build tf.data.Dataset\n",
    "from numpy arrays\n",
    "\"\"\"\n",
    "batch_size = 64\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "train = train.shuffle(1024).batch(batch_size).repeat()\n",
    "print('train dataset built')\n",
    "val = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
    "val = val.shuffle(1024).batch(batch_size)\n",
    "print('validation dataset built')\n",
    "test = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "test = test.shuffle(1024).batch(batch_size)\n",
    "print('test dataset built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "av0p6ZlufaFp"
   },
   "outputs": [],
   "source": [
    "train_mean = np.mean(train,(0,2)).reshape(1,105,1)\n",
    "train_std = np.std(train,(0,2)).reshape(1,105,1)\n",
    "labels_mean = np.mean(labels)\n",
    "labels_std = np.std(labels)\n",
    "\n",
    "train = (train - train_mean) / train_std\n",
    "labels = (labels - labels_mean) / labels_std\n",
    "\n",
    "test_size = np.floor(test_split*len(train))\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((train,labels))\n",
    "train_set = train_set.shuffle(10000)\n",
    "test_set = train_set.take(test_size).batch(256)\n",
    "train_set = train_set.skip(test_size)\n",
    "train_set = train_set.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here is the defintion of some callbacks\n",
    "\"\"\"\n",
    "\n",
    "# tensorboard callback\n",
    "log_dir = '.\\\\logs\\\\fit\\\\' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# checkpoint callback\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath='.\\\\checkpoints\\\\model_{epoch}',\n",
    "                                                      save_best_only=True,\n",
    "                                                      monitor='val_loss',\n",
    "                                                      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building and training a model\n",
    "\"\"\"\n",
    "lstm1_out = 30\n",
    "dense1_out = 100\n",
    "dense2_out = 1\n",
    "num_epoch = 10\n",
    "alpha = 0.001\n",
    "reg = 0.1\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.LSTM(lstm1_out, input_shape=train_x[0].shape, activation='relu', kernel_regularizer=keras.regularizers.l2(l=reg)),\n",
    "  keras.layers.Dense(dense1_out, kernel_regularizer=keras.regularizers.l2(l=reg)),\n",
    "  keras.layers.Dense(dense2_out, kernel_regularizer=keras.regularizers.l2(l=reg))\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=alpha),\n",
    "              loss='mse',\n",
    "              metrics=['mse', 'mae'])\n",
    "history = model.fit(train, validation_data=val, epochs=num_epoch, steps_per_epoch=1,\n",
    "                    callbacks=[tensorboard_callback, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuLZHwQy7O5Y"
   },
   "outputs": [],
   "source": [
    "# !!!! 107 rows which one is for close prices?\n",
    "\n",
    "\"\"\"\n",
    "In this section we randomly plot the results of\n",
    "predictions on 1 example from both validation and test sets. \n",
    "\"\"\"\n",
    "\n",
    "val_sample = rnd.randint(1, val_x[0].shape[0] - 1)\n",
    "x1 = val_x[val_sample, :, :].reshape(1, val_x.shape[1], -1)\n",
    "y1 = val_y[val_sample]\n",
    "train_sample = rnd.randint(1, train_x[0].shape[0] - 1)\n",
    "x2 = train_x[train_sample, :, :].reshape(1, train_x.shape[1], -1)\n",
    "y2 = train_y[train_sample]\n",
    "\n",
    "yprime1 = model.predict(x1)\n",
    "yprime2 = model.predict(x2)\n",
    "\n",
    "x_axis = [i for i in range(w + 1)]\n",
    "fig, a = plt.subplots(2, 1)\n",
    "a[0].plot(x_axis[:-1], x1[0, 0, :], 'bo', label='history')\n",
    "a[0].plot(x_axis[-1:], y1, 'ro', label='real')\n",
    "a[0].plot(x_axis[-1:], yprime1, 'go', label='prediction')\n",
    "a[0].legend()\n",
    "a[0].set_title('Validation Set')\n",
    "a[1].plot(x_axis[:-1], x2[0, 0, :], 'bo', label='history')\n",
    "a[1].plot(x_axis[-1:], y2, 'ro', label='real')\n",
    "a[1].plot(x_axis[-1:], yprime2, 'go', label='prediction')\n",
    "a[1].legend()\n",
    "a[1].set_title('Train Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "dataset_creator.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
