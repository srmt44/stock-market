{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "corr_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srmt99/stock-market/blob/master/corr_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nkhuz0qrc4Xi"
      },
      "source": [
        "# **Mount google drive** (colab only)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P2WbF-OvTj0t",
        "colab": {}
      },
      "source": [
        "# run this cell only when using colab\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "ROOT = '/content/drive'\n",
        "print(ROOT)\n",
        "drive.mount(ROOT, force_remount=True)\n",
        "\n",
        "%mkdir '/content/drive/My Drive/stock-market'\n",
        "%cd '/content/drive/My Drive/stock-market'\n",
        "%mkdir 'data/'\n",
        "%mkdir 'all_records/'\n",
        "%ls\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfhmBL4CHEgP",
        "colab_type": "text"
      },
      "source": [
        "# **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szvn9uZ1vR6b",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorflow import keras\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import glob\n",
        "import os\n",
        "import copy\n",
        "import random as rnd\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "mpl.rcParams['figure.figsize'] = (6, 4)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmYQgsPOHEgd",
        "colab_type": "text"
      },
      "source": [
        "# **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "colab_type": "code",
        "id": "9Rt2JhZcvR6l",
        "colab": {}
      },
      "source": [
        "# data extraction\n",
        "plt.figure(figsize=(10,6))\n",
        "test_split = 0.2\n",
        "future_prediction = 1\n",
        "corr_w = 40\n",
        "w = 30\n",
        "k = 10\n",
        "corr_input = True\n",
        "val_split, test_split = 0.1, 0.2\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vK6C_9UveHBz"
      },
      "source": [
        "# **Download data** (colab only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44VRqM8YUH_V",
        "colab": {}
      },
      "source": [
        "# uploading the data from github into colab's working space\n",
        "%cd 'data'\n",
        "!wget https://github.com/srmt99/stock-market/blob/master/data/markets.npy?raw=true -O markets.npy\n",
        "!wget https://github.com/srmt99/stock-market/blob/master/data/stocks_1.npy?raw=true -O stocks_1.npy\n",
        "!wget https://github.com/srmt99/stock-market/blob/master/data/stocks_2.npy?raw=true -O stocks_2.npy\n",
        "%cd ..\n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiXym33dHEg4",
        "colab_type": "text"
      },
      "source": [
        "# **Reading stocks and markets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jiC6k25WvR6q",
        "colab": {}
      },
      "source": [
        "stocks = []\n",
        "for i in np.load(\"data/stocks_1.npy\",allow_pickle=True):\n",
        "    stocks.append(i)\n",
        "for i in np.load(\"data/stocks_2.npy\",allow_pickle=True):\n",
        "    stocks.append(i)\n",
        "stocks = np.array(stocks)\n",
        "markets = np.load(\"data/markets.npy\",allow_pickle=True)\n",
        "                  \n",
        "# extracting minimum market length\n",
        "min_market_len = len(markets[0])\n",
        "for i in markets[1:]:\n",
        "    if len(i)<min_market_len:\n",
        "        min_market_len = len(i)\n",
        "\n",
        "# making all markets of the same length\n",
        "for i in range(len(markets)):\n",
        "    while len(markets[i])>min_market_len:\n",
        "        markets[i] = np.delete(markets[i],0,0)\n",
        "\n",
        "for i in range(len(stocks)):\n",
        "    while len(stocks[i])>min_market_len:\n",
        "        stocks[i] = np.delete(stocks[i],0,0)\n",
        "\n",
        "markets = np.stack(markets,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ff-MgLLHEhF",
        "colab_type": "text"
      },
      "source": [
        "# **Plot random stock and market**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kx8QdzbTvR6u",
        "colab": {}
      },
      "source": [
        "# ploting some random stock prices and markets\n",
        "r1 = np.random.randint(len(stocks))\n",
        "r2 = np.random.randint(len(markets))\n",
        "print(r1,r2)\n",
        "plt.figure(figsize=(20,6))\n",
        "plt.plot(stocks[r1][:,1],label=\"stock prices\")\n",
        "plt.plot(markets[r2][:,1],label=\"market\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydjTJ2JiHEhN",
        "colab_type": "text"
      },
      "source": [
        "# **Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_sQPbd2vR6z",
        "colab": {}
      },
      "source": [
        "def turn_into_windows(input,kernel):\n",
        "    \"\"\"\n",
        "    this function is used to make windows out of the input array and select a label for each window\n",
        "    input array of shape: (len_input, num_features) is turned into windows of shape: (kernel,num_features)\n",
        "    and a labels is selected for each window , which is a floating point number label is selected\n",
        "    from the second features (arr[...,1]) which here is <CLOSE> price for the NEXT sample\n",
        "    NOTE: every window and it's label is normilized together and the info for denormilizing it is stored in\n",
        "    norm_info, which at this time is not needed therfore is commented.\n",
        "\n",
        "    input: 2D array of (time_samples, features)\n",
        "    kernel: window size\n",
        "\n",
        "    returns: two np.arrays of shape: (len_input - kernel + 1, kernel, num_features) , (len_input - kernel + 1,)\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    labels = []\n",
        "    # norm_info = []\n",
        "    # j = 0\n",
        "    # j is for counting num(windows) , used for norm_info\n",
        "    for i in range(kernel,len(input)-1):\n",
        "        window = input[i-kernel:i+1,:]\n",
        "        # norm_info.append([np.min(window,0),None])\n",
        "        window = window - np.min(window,0)\n",
        "        # norm_info[j][1] = np.max(window,0)+0.0001\n",
        "        window = window / (np.max(window,0)+0.0001)\n",
        "        data.append(window[:-1,:])\n",
        "        labels.append(window[-1,1])\n",
        "        # j+=1\n",
        "    return np.array(data),np.array(labels).reshape(len(labels),1)\n",
        "\n",
        "def turn_into_windows_multi(input,kernel):\n",
        "    \"\"\"\n",
        "    this function is mush similar to the one above (turn_into_windows)\n",
        "    yet this fucntion was created speceficaly for extracting 2d matrices from all markets as windows.\n",
        "    meaning at each step, we make <num_markets> windows from all market <CLOSE> prices together, which\n",
        "    will increase the speed of creating the records further.\n",
        "    input is a 2d matrix of shape: (num_markets, len_markets) and is turned into 2d matrices\n",
        "    of shape: (num_markets, kernel)\n",
        "    NOTE: all of the data in each matrix of windows, is normilized. the info is stored in norm_info\n",
        "    which is commented at the time being , since there is no use for it.\n",
        "\n",
        "    input: 2d matrix of shape: (num_markets, time_samples)\n",
        "    kernel: window size\n",
        "\n",
        "    returns: a np.array of shape: (len_markets - kernel + 1, num_markets, kernel)\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    norm_info = []\n",
        "    # j = 0\n",
        "    # j is for counting num(windows) , used for norm_info\n",
        "    for i in range(kernel,input.shape[1]-1):\n",
        "        window = input[:,i-kernel:i]\n",
        "        # norm_info.append([np.min(window,1).reshape((len(window),1)),None])\n",
        "        window = window - np.min(window,1).reshape((len(window),1))\n",
        "        # norm_info[j][1] = np.max(window,1).reshape((len(window),1))+0.0001\n",
        "        window = window / (np.max(window,1).reshape((len(window),1))+0.0001)\n",
        "        data.append(window)\n",
        "    return np.array(data)\n",
        "\n",
        "def smooth(input):\n",
        "    output = []\n",
        "    output.append(input[0])\n",
        "    output.append(np.mean([input[0],input[1]]))\n",
        "    for i in range(2,len(input)-2):\n",
        "        mean = np.mean([input[i-2],input[i-1],input[i],input[i+1],input[i+2]])\n",
        "        output.append(mean)\n",
        "    output.append(np.mean([input[len(input)-2],input[len(input)-1]]))\n",
        "    output.append(input[len(input)-1])\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjWmzjIHEhU",
        "colab_type": "text"
      },
      "source": [
        "# **Building and saving records**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mPdHPNqEvR63",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "this part, creates the records and saves them as np.array into files\n",
        "it itreates over stocks, one stock at a time, the stock is first turend into windows\n",
        "of shape: (corr_w,) holding <CLOSE> price in a list of windows. these windows are used\n",
        "to calculate the correlation with the coresponding market windows of shape: (num_mrkets,corr_w)\n",
        "meaning the correalation of each stock window is calculataed against all markets at the same time\n",
        "then, both the stock and markets are turned into windows of length <w> to be put into the records\n",
        "togrther with the before calculated correlations.\n",
        "\"\"\"\n",
        "for count,stock in enumerate(stocks):\n",
        "    records = []\n",
        "    labels = []\n",
        "    correlations = []\n",
        "    x,y = turn_into_windows(stock,corr_w)\n",
        "    for wc,window in enumerate(x[:,:,1]):\n",
        "        corr = np.corrcoef(window,markets[: , min_market_len - len(x) - corr_w - 1 + wc : min_market_len - len(x) + wc -1 , 1])[1:,0]\n",
        "        correlations.append( np.nan_to_num(corr) )\n",
        "    correlations = np.array(correlations)\n",
        "    x,y = turn_into_windows(stock,w)\n",
        "    x2 = turn_into_windows_multi(markets[:,:,1],w)\n",
        "    for wc in range(len(correlations),w,-1):\n",
        "        record = np.zeros( (2*len(markets)+5,w) )\n",
        "        record[:5,:] = np.transpose(x[wc + (len(x)-len(correlations)-1) ][:,1:]) # part 1\n",
        "        record[5:5+len(markets)] = np.transpose(correlations[wc-w:wc]) # part 2\n",
        "        record[5+len(markets):5+2*len(markets)] = x2[wc + (len(x2)-len(correlations)-1)] # part 3\n",
        "        records.append(record)\n",
        "        labels.append(y[wc + (len(x)-len(correlations)-1) ])\n",
        "    np.save(f\"all_records/records_{count}\",np.array([records,labels]))\n",
        "    print(f\"{count}/{len(stocks)}\")\n",
        "print(\"__done__\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFYQp-pWHEha",
        "colab_type": "text"
      },
      "source": [
        "# **Reading records**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRv4HKZPvR67",
        "colab": {}
      },
      "source": [
        "train = []\n",
        "labels = []\n",
        "for count,filename in enumerate(glob.glob(\"all_records/records_*.npy\")):\n",
        "    x,y = np.load(filename,allow_pickle=True)\n",
        "    for i in x:\n",
        "        train.append(i)\n",
        "    for i in y:\n",
        "        labels.append(i[0])\n",
        "\n",
        "corr_input = True\n",
        "train = np.array(train)\n",
        "labels = np.array(labels)\n",
        "print(\"train set shape:\",train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCHIBPnSZJLk",
        "colab_type": "text"
      },
      "source": [
        "# **keeping k corr+makets only**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ZlmRlNZL_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_largest_train = []\n",
        "for rec in train:\n",
        "  new_rec = np.zeros((5+2*k,30))\n",
        "  index=np.argsort(np.sum(abs(rec[5:56]),1))[-10:]\n",
        "  new_rec[:5,:] = rec[:5,:]\n",
        "  new_rec[5:5+k,:] = rec[index+5]\n",
        "  new_rec[5+k:5+2*k] = rec[index+5+51]\n",
        "  k_largest_train.append(new_rec)\n",
        "k_largest_train = np.array(k_largest_train)\n",
        "train = np.copy(k_largest_train)\n",
        "print(\"new train set shape:\",train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCKJZbmdZUHD",
        "colab_type": "text"
      },
      "source": [
        "# **cutting the correlation out** (For NON-Corr input only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r19sEsOMZUkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_corr_train = []\n",
        "for rec in train:\n",
        "  new_rec = np.zeros((5+k,30))\n",
        "  new_rec[:5,:] = rec[:5,:]\n",
        "  new_rec[5:5+k,:] = rec[5+k:5+2*k,:]\n",
        "  non_corr_train.append(new_rec)\n",
        "non_corr_train = np.array(non_corr_train)\n",
        "train = non_corr_train\n",
        "corr_input = False\n",
        "print(\"new train set shape:\",train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED3SkvuXZgj7",
        "colab_type": "text"
      },
      "source": [
        "# **inserting CLOSE in between**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StrBinbGZhBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "close_inserted_train = []\n",
        "if corr_input:\n",
        "  num_markets = int((train[0].shape[0]-5)/2)\n",
        "  for rec in train:\n",
        "    new_rec = np.zeros((5+3*num_markets,30))\n",
        "    new_rec[:5,:] = rec[:5,:]\n",
        "    new_rec[range(5,len(new_rec),3),:] = rec[0,:]\n",
        "    new_rec[range(6,len(new_rec),3),:] = rec[5:5+num_markets,:]\n",
        "    new_rec[range(7,len(new_rec),3),:] = rec[5+num_markets:5+2*num_markets,:]\n",
        "    close_inserted_train.append(new_rec)\n",
        "  close_inserted_train = np.array(close_inserted_train)\n",
        "  train = close_inserted_train\n",
        "  corr_input = False\n",
        "else:\n",
        "  num_markets = int(train[0].shape[0]-5)\n",
        "  for rec in train:\n",
        "    new_rec = np.zeros((5+2*num_markets,30))\n",
        "    new_rec[:5,:] = rec[:5,:]\n",
        "    new_rec[range(5,len(new_rec),2),:] = rec[0,:]\n",
        "    new_rec[range(6,len(new_rec),2),:] = rec[5:5+num_markets,:]\n",
        "    close_inserted_train.append(new_rec)\n",
        "  close_inserted_train = np.array(close_inserted_train)\n",
        "  train = close_inserted_train\n",
        "  corr_input = False\n",
        "\n",
        "print(\"new train set shape:\",train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4QLo4JVHEhf",
        "colab_type": "text"
      },
      "source": [
        "# **Train, validation and test data preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EnpGMs-GvR6-",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "We split data into 3 parts: train, validation, test\n",
        "We control the number of records in each set using\n",
        "test_split and val_split\n",
        "\"\"\"\n",
        "data = train\n",
        "\n",
        "mask = np.arange(data.shape[0])\n",
        "np.random.shuffle(mask)\n",
        "data = data[mask]\n",
        "labels = labels[mask]\n",
        "\n",
        "num_val, num_test = int(val_split * data.shape[0]), int(test_split * data.shape[0])\n",
        "\n",
        "train_x = data[:-(num_val + num_test)]\n",
        "train_y = labels[:-(num_val + num_test)]\n",
        "val_x = data[-(num_val + num_test):-num_test]\n",
        "val_y = labels[-(num_val + num_test):-num_test]\n",
        "test_x = data[-num_test:]\n",
        "test_y = labels[-num_test:]\n",
        "\n",
        "train_x[np.isnan(train_x)] = 0\n",
        "train_y[np.isnan(train_y)] = 0\n",
        "val_x[np.isnan(val_x)] = 0\n",
        "val_y[np.isnan(val_y)] = 0\n",
        "test_x[np.isnan(test_x)] = 0\n",
        "test_y[np.isnan(test_y)] = 0\n",
        "\n",
        "print('train_x:', train_x.shape)\n",
        "print('train_y:', train_y.shape)\n",
        "print('val_x:', val_x.shape)\n",
        "print('val_y:', val_y.shape)\n",
        "print('test_x:', test_x.shape)\n",
        "print('test_y:', test_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X2eVbFaHEhj",
        "colab_type": "text"
      },
      "source": [
        "# **Building tf.data.Dataset from numpy arrays**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TYAsM6K5vR7E",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "In this cell we build tf.data.Dataset\n",
        "from numpy arrays\n",
        "\"\"\"\n",
        "train = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
        "train = train.shuffle(1024).batch(batch_size).repeat()\n",
        "print('train dataset built')\n",
        "val = tf.data.Dataset.from_tensor_slices((val_x, val_y))\n",
        "val = val.shuffle(1024).batch(batch_size).repeat()\n",
        "print('validation dataset built')\n",
        "test = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
        "test = test.shuffle(1024).batch(batch_size)\n",
        "print('test dataset built')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrYhfPDXHEho",
        "colab_type": "text"
      },
      "source": [
        "# **Callback definitions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b0hQtw9O0vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DirectionCheck(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, x, y, num):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.num = num\n",
        "        self.mask = list(range(x.shape[0]))\n",
        "\n",
        "    def cal_direction_metrics(self, x, y, yprime):\n",
        "        a = y - x\n",
        "        b = yprime - x\n",
        "\n",
        "        a2 = a.copy()\n",
        "        b2 = b.copy()\n",
        "        a2[a2 < 0] = 0\n",
        "        a2[a2 > 0] = 1\n",
        "        b2[b2 < 0] = 0\n",
        "        b2[b2 > 0] = 1\n",
        "        up = a2.sum()\n",
        "        p_up = (a2 * b2).sum()\n",
        "\n",
        "        a2 = a.copy()\n",
        "        b2 = b.copy()\n",
        "        a2[a2 > 0] = 0\n",
        "        a2[a2 < 0] = 1\n",
        "        b2[b2 > 0] = 0\n",
        "        b2[b2 < 0] = 1\n",
        "        down = a2.sum()\n",
        "        p_down = (a2 * b2).sum()\n",
        "\n",
        "        return up, p_up, down, p_down\n",
        "\n",
        "    def on_epoch_end(self, batch, logs=None):\n",
        "        np.random.shuffle(self.mask)\n",
        "        mask = self.mask[:self.num]\n",
        "        x = self.x[mask]\n",
        "        y = self.y[mask]\n",
        "        yprime = self.model.predict(x)\n",
        "        up, p_up, down, p_down = self.cal_direction_metrics(x[:, 0, -1], y, yprime[:, 0])\n",
        "        print('\\nup predicted / up: {} / {} - down predicted / down: {} / {}\\n'.format(p_up, up, p_down, down))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F5Y3v_I4vR7N",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Here is the defintion of some callbacks\n",
        "\"\"\"\n",
        "\n",
        "# tensorboard callback\n",
        "log_dir = Path('./logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# checkpoint callback\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath='./checkpoints/model_{epoch}',\n",
        "                                                      save_best_only=True,\n",
        "                                                      monitor='val_loss',\n",
        "                                                      verbose=0)\n",
        "# earlystopping callback\n",
        "earlystopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                          min_delta=1e-4,\n",
        "                                                          patience=8,\n",
        "                                                          restore_best_weights=True,)\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                 min_delta=1e-4,\n",
        "                                                 factor=0.4,\n",
        "                                                 patience=2, min_lr=1e-5, verbose=1)\n",
        "\n",
        "direction_callback = DirectionCheck(val_x, val_y, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfxJUNpvHEht",
        "colab_type": "text"
      },
      "source": [
        "# **Building, training, evaluating model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OTxvdw3e7-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -r logs # delete any logs so far"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKeeNTWNvR7Q",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Building and training a model\n",
        "\"\"\"\n",
        "# model params\n",
        "lstm1_out = 32\n",
        "lstm2_out = 64\n",
        "dense1_out = 32\n",
        "dense2_out = 1\n",
        "num_epoch = 1000\n",
        "alpha = 1e-4\n",
        "reg = 0\n",
        "\n",
        "model = keras.Sequential([\n",
        "  keras.layers.LSTM(lstm1_out, input_shape=train_x[0].shape, kernel_regularizer=keras.regularizers.l1_l2(l1=reg,l2=reg), return_sequences=True),\n",
        "  keras.layers.LSTM(lstm2_out, kernel_regularizer=keras.regularizers.l1_l2(l1=reg,l2=reg)),\n",
        "  keras.layers.Dense(dense1_out, activation='relu', kernel_regularizer=keras.regularizers.l1_l2(l1=reg,l2=reg)),\n",
        "  keras.layers.Dense(dense2_out, activation='sigmoid', kernel_regularizer=keras.regularizers.l1_l2(l1=reg,l2=reg))\n",
        "])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=alpha),\n",
        "              loss='mse',\n",
        "              metrics=['mse', 'mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFViV2fyb8Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(train, validation_data=val, validation_steps=630, epochs=num_epoch, steps_per_epoch=4416,\n",
        "                    callbacks=[tensorboard_callback,\n",
        "                               checkpoint_callback,\n",
        "                               earlystopping_callback,\n",
        "                               reduce_lr,\n",
        "                               direction_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4npJebQvUY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"evaluating on test set\")\n",
        "t = model.evaluate(test,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWslxX_zdCYn",
        "colab_type": "text"
      },
      "source": [
        "# **Saving model parameters and results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqtZnOwWc-zB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open(\"results.txt\",\"a\")\n",
        "loss = history.history[\"loss\"][-1]\n",
        "val_loss = history.history[\"val_loss\"][-1]\n",
        "string = f\"l_o={lstm1_out},d1_o={dense1_out},d2_o={dense2_out},e={num_epoch},a={alpha},r={reg}:l={str(loss)[:6]},v_l={str(val_loss)[:6]},t_l={str(t[0])[:6]}\\n\"\n",
        "f.write(string)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWd68gHGp39t",
        "colab_type": "text"
      },
      "source": [
        "# **Launch TensorBoard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLNNBfHyp0qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_base_dir = \"./logs\"\n",
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOH2e_c0HEhx",
        "colab_type": "text"
      },
      "source": [
        "# **Plot random validation and train data predictinos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kuLZHwQy7O5Y",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "In this section we randomly plot the results of\n",
        "predictions on 1 example from both validation and test sets. \n",
        "\"\"\"\n",
        "\n",
        "val_sample = rnd.randint(1, val_x.shape[0] - 1)\n",
        "x1 = val_x[val_sample, :, :].reshape(1, val_x.shape[1], -1)\n",
        "y1 = val_y[val_sample]\n",
        "train_sample = rnd.randint(1, train_x.shape[0] - 1)\n",
        "x2 = train_x[train_sample, :, :].reshape(1, train_x.shape[1], -1)\n",
        "y2 = train_y[train_sample]\n",
        "test_sample = rnd.randint(1, test_x.shape[0] - 1)\n",
        "x3 = test_x[test_sample, :, :].reshape(1, test_x.shape[1], -1)\n",
        "y3 = test_y[test_sample]\n",
        "\n",
        "yprime1 = model.predict(x1)\n",
        "yprime2 = model.predict(x2)\n",
        "yprime3 = model.predict(x3)\n",
        "\n",
        "x_axis = [i for i in range(w + 1)]\n",
        "fig, a = plt.subplots(3, 1)\n",
        "fig.set_figheight(8)\n",
        "fig.set_figwidth(8)\n",
        "a[0].plot(x_axis[:-1], x1[0, 0, :], 'bo', label='history')\n",
        "a[0].plot(x_axis[-1:], y1, 'ro', label='real')\n",
        "a[0].plot(x_axis[-1:], yprime1, 'go', label='prediction')\n",
        "a[0].legend()\n",
        "a[0].set_title('Validation Set')\n",
        "a[1].plot(x_axis[:-1], x2[0, 0, :], 'bo', label='history')\n",
        "a[1].plot(x_axis[-1:], y2, 'ro', label='real')\n",
        "a[1].plot(x_axis[-1:], yprime2, 'go', label='prediction')\n",
        "a[1].legend()\n",
        "a[1].set_title('Train Set')\n",
        "a[2].plot(x_axis[:-1], x3[0, 0, :], 'bo', label='history')\n",
        "a[2].plot(x_axis[-1:], y3, 'ro', label='real')\n",
        "a[2].plot(x_axis[-1:], yprime3, 'go', label='prediction')\n",
        "a[2].legend()\n",
        "a[2].set_title('Test Set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL-_kqWjIRoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}